{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def find_and_copy_fits_files(src_folder):\n",
    "    # Create the target directory if it doesn't exist\n",
    "    target_folder = os.path.join(src_folder, 'm101_only')\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    # Walk through all the folders within the source folder\n",
    "    for root, dirs, files in os.walk(src_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.fit') and ('m101' in file.lower()):\n",
    "                source_file_path = os.path.join(root, file)\n",
    "                destination_file_path = os.path.join(target_folder, file)\n",
    "                \n",
    "                # Copy the file to the target directory\n",
    "                shutil.copy2(source_file_path, destination_file_path)\n",
    "                print(f'Copied {file} to {target_folder}')\n",
    "\n",
    "# Define the source folder\n",
    "source_folder = 'D:\\Astronomy-Image-Processing\\m51\\2024-07-26+27'\n",
    "\n",
    "# Call the function\n",
    "find_and_copy_fits_files(source_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from astropy.io import fits\n",
    "from astropy.nddata import CCDData\n",
    "import ccdproc\n",
    "import astroalign as aa\n",
    "from astroscrappy import detect_cosmics\n",
    "import re\n",
    "\n",
    "def load_all_fits_images(root_folder):\n",
    "    \"\"\"Recursively load all FITS files from a specified root folder and return both the HDUList and filenames.\"\"\"\n",
    "    files = []\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.fit'):\n",
    "                files.append(os.path.join(dirpath, f))\n",
    "    files.sort(key=lambda x: int(re.findall(r'\\d+', x.split('_')[-1].split('.')[0])[0]))  # Sort numerically\n",
    "    return [(fits.open(file), file) for file in files]\n",
    "\n",
    "def calibrate_and_save_images(science_images, flat_images, bias_images, output_folder):\n",
    "    \"\"\"Calibrate science images using provided flat and bias frames, perform cosmic ray removal, realign images, stack all images together, and save the calibrated images.\"\"\"\n",
    "    # Debug: Print the number of loaded bias and flat images\n",
    "    print(f\"Number of bias images: {len(bias_images)}\")\n",
    "    print(f\"Number of flat images: {len(flat_images)}\")\n",
    "    \n",
    "    bias_data = [CCDData(img[0].data, unit='adu') for img, _ in bias_images]\n",
    "    combined_bias = ccdproc.combine(bias_data, method='median')\n",
    "\n",
    "    flat_data = [CCDData(img[0].data, unit='adu') for img, _ in flat_images]\n",
    "    corrected_flats = [ccdproc.subtract_bias(flat, combined_bias) for flat in flat_data]\n",
    "    \n",
    "    # Debug: Check if flat images are processed correctly\n",
    "    if not corrected_flats:\n",
    "        print(\"No corrected flat images found.\")\n",
    "    \n",
    "    for flat in corrected_flats:\n",
    "        cosmic_mask, clean_data = detect_cosmics(flat.data, gain=1.0, readnoise=10.0, sigclip=5.0)\n",
    "        flat.data = clean_data\n",
    "\n",
    "    normalized_flats = [flat.divide(np.median(flat.data[flat.data > 0])) for flat in corrected_flats]\n",
    "    \n",
    "    # Debug: Check if normalization was successful\n",
    "    print(f\"Number of normalized flats: {len(normalized_flats)}\")\n",
    "    if not normalized_flats:\n",
    "        print(\"Normalization of flat images failed. Exiting.\")\n",
    "        return []\n",
    "    \n",
    "    combined_flat = ccdproc.combine(normalized_flats, method='median')\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    science_ccds = [CCDData(img[0].data, unit='adu', meta=img[0].header.copy()) for img, _ in science_images]\n",
    "\n",
    "    corrected_science_ccds = []\n",
    "    reference_image = None\n",
    "    exposure_time = science_images[0][0][0].header['EXPTIME']\n",
    "    print(f\"Exposure time of each image: {exposure_time} seconds\")\n",
    "\n",
    "    stack_data = np.zeros_like(science_ccds[0].data, dtype='float32')\n",
    "    stack_header = science_ccds[0].meta\n",
    "\n",
    "    successful_stacks = 0\n",
    "    failed_stacks = 0\n",
    "\n",
    "    for (science_ccd_hdulist, filename) in science_images:\n",
    "        try:\n",
    "            # Ensure we are working with CCDData\n",
    "            science_ccd = CCDData(science_ccd_hdulist[0].data, unit='adu', meta=science_ccd_hdulist[0].header.copy())\n",
    "\n",
    "            corrected_ccd = ccdproc.subtract_bias(science_ccd, combined_bias)\n",
    "            corrected_ccd = ccdproc.flat_correct(corrected_ccd, combined_flat)\n",
    "\n",
    "            cosmic_mask, clean_data = detect_cosmics(corrected_ccd.data, gain=1.0, readnoise=10.0, sigclip=5.0, sigfrac=0.3)\n",
    "            corrected_ccd.data = clean_data\n",
    "\n",
    "            if reference_image is None:\n",
    "                reference_image = corrected_ccd\n",
    "\n",
    "            # Enhanced alignment process\n",
    "            try:\n",
    "                aligned_data, _ = aa.register(corrected_ccd.data, reference_image.data, max_control_points=150, detection_sigma=2.5, min_area=4)\n",
    "            except Exception as alignment_error:\n",
    "                print(f\"Retrying alignment for image {filename} with adjusted parameters due to error: {alignment_error}\")\n",
    "                aligned_data, _ = aa.register(corrected_ccd.data, reference_image.data, max_control_points=100, detection_sigma=3.5, min_area=6)\n",
    "\n",
    "            aligned_ccd = CCDData(aligned_data, unit='adu', meta=corrected_ccd.meta)\n",
    "            stack_data += aligned_ccd.data\n",
    "            successful_stacks += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping image {filename} due to error: {e}\")\n",
    "            failed_stacks += 1\n",
    "\n",
    "    if successful_stacks > 0:\n",
    "        stack_data /= successful_stacks\n",
    "        stacked_ccd = CCDData(stack_data.astype('float32'), unit='adu', meta=stack_header)\n",
    "        corrected_science_ccds.append(stacked_ccd)\n",
    "\n",
    "        output_file = os.path.join(output_folder, 'stacked_image.fits')\n",
    "        fits.writeto(output_file, stacked_ccd.data, stacked_ccd.meta, overwrite=True)\n",
    "\n",
    "    print(f\"Number of successfully stacked images: {successful_stacks}\")\n",
    "    print(f\"Number of images that failed to stack: {failed_stacks}\")\n",
    "\n",
    "    return corrected_science_ccds\n",
    "\n",
    "# Example usage--------------------------------------------------------SET FOLDER HERE\n",
    "science_folder = \"D:\\\\Astronomy-Image-Processing\\\\m101\\\\m101_r\"\n",
    "flat_folder = \"D:\\\\Astronomy-Image-Processing\\\\m101\\\\flat_r\"\n",
    "bias_folder = \"D:\\\\Astronomy-Image-Processing\\\\m101\\\\bias\"\n",
    "output_folder = \"D:\\\\Astronomy-Image-Processing\\\\m101\\\\calibrated_images_m101\"\n",
    "\n",
    "flat_images = load_all_fits_images(flat_folder)\n",
    "bias_images = load_all_fits_images(bias_folder)\n",
    "science_images = load_all_fits_images(science_folder)\n",
    "\n",
    "corrected_science_ccds = calibrate_and_save_images(science_images, flat_images, bias_images, output_folder)\n",
    "print(\"Calibration and stacking complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "NewRandomAccessFile failed to Create/Open: path_to_your_greyscale_image.jpg : The system cannot find the file specified.\r\n; No such file or directory [Op:ReadFile]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model(image)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load and preprocess your greyscale image\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_image(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_your_greyscale_image.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mgrayscale_to_rgb(image)  \u001b[38;5;66;03m# Convert to 3 channels as required by the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aiden\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\io_ops.py:137\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.read_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(filename, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    102\u001b[0m   \u001b[38;5;124;03m\"\"\"Reads the contents of file.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m  This operation returns a tensor with the entire contents of the input\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    A tensor of dtype \"string\", with the file contents.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_io_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aiden\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:565\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(filename, name)\u001b[0m\n\u001b[0;32m    563\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_file_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m    568\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aiden\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:588\u001b[0m, in \u001b[0;36mread_file_eager_fallback\u001b[1;34m(filename, name, ctx)\u001b[0m\n\u001b[0;32m    586\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [filename]\n\u001b[0;32m    587\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 588\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[0;32m    591\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[0;32m    592\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadFile\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32mc:\\Users\\Aiden\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNotFoundError\u001b[0m: NewRandomAccessFile failed to Create/Open: path_to_your_greyscale_image.jpg : The system cannot find the file specified.\r\n; No such file or directory [Op:ReadFile]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load the pre-trained ESRGAN model from TensorFlow Hub\n",
    "model = hub.load(\"https://tfhub.dev/captain-pool/esrgan-tf2/1\")\n",
    "\n",
    "def enhance_image(image):\n",
    "    return model(image)\n",
    "\n",
    "# Load and preprocess your greyscale image\n",
    "image = tf.image.decode_image(tf.io.read_file('path_to_your_greyscale_image.jpg'), channels=1)\n",
    "image = tf.image.grayscale_to_rgb(image)  # Convert to 3 channels as required by the model\n",
    "image = tf.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "# Enhance the image\n",
    "enhanced_image = enhance_image(image)\n",
    "\n",
    "# Convert back to greyscale and save the enhanced image\n",
    "enhanced_image = tf.image.rgb_to_grayscale(tf.squeeze(enhanced_image))\n",
    "tf.io.write_file('enhanced_image.jpg', tf.image.encode_jpeg(enhanced_image))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
